{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c0e3cb4-6b79-4d65-872b-8e0a763f8370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "env_path = os.path.join(Path.cwd().parent, \".env\")\n",
    "load_dotenv(env_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ce78146-82c6-453f-b8c4-fdf6001f17ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aum/Desktop/Programming/llm/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88ac4857-b465-4f88-9f3f-63d2cf162f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantization_config = BitsAndBytesConfig(load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13948b4-ab1d-4400-b268-cadc69495f02",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d221a5f-4c3e-44e4-96bc-2f598187abcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorize_text(text):\n",
    "    for word, color in zip([\"Instruction\"], [\"blue\"]):\n",
    "        text = text.replace(f\"{word}:\\n\", f\"**<font color='{color}'>{word}:</font>**\\n\\n\")\n",
    "    for word, color in zip([\"Response\"], [\"green\"]):\n",
    "        text = text.replace(f\"\\n\\n{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
    "    return text\n",
    "template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75d89e6-3983-4081-bace-f6fe1d779a95",
   "metadata": {},
   "source": [
    "### CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3da203be-72e1-41f3-8e1d-f6937dd165f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded in 12.508863925933838s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\",\n",
    "                                          # device_map=\"auto\",\n",
    "                                          token=os.environ.get(\"hugging_face_token\"))\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\",\n",
    "                                             # device_map=\"auto\",\n",
    "                                             token=os.environ.get(\"hugging_face_token\"),\n",
    "                                             use_cache=True\n",
    "                                             # quantization_config=quantization_config\n",
    "                                            )\n",
    "print(f\"Model loaded in {time.time() - start_time}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29f71ba4-a8c9-4bba-a976-ee0a5a3a5e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"What is coroutine in Python? Explain with an example.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")#.to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d71c8c2-0248-470d-bf89-1419f6ef9e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference completed and output generated in 449.9799690246582s\n"
     ]
    }
   ],
   "source": [
    "inf_start_time = time.time()\n",
    "outputs = model.generate(**input_ids, max_length=1000)\n",
    "print(f\"Inference completed and output generated in {time.time() - inf_start_time}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db700a72-8f5c-43a0-9958-bb4077b41b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<bos>What is coroutine in Python? Explain with an example.\n",
       "\n",
       "Coroutine is a special type of function in Python that allows you to define a function that can be paused and resumed later. This makes it possible to perform long-running tasks without blocking the main thread.\n",
       "\n",
       "Here's an example of how to use coroutines:\n",
       "\n",
       "```python\n",
       "import asyncio\n",
       "\n",
       "async def my_coroutine():\n",
       "    print(\"Starting coroutine...\")\n",
       "    await asyncio.sleep(2)\n",
       "    print(\"Coroutine finished!\")\n",
       "\n",
       "asyncio.run(my_coroutine())\n",
       "```\n",
       "\n",
       "**Output:**\n",
       "\n",
       "```\n",
       "Starting coroutine...\n",
       "Coroutine finished!\n",
       "```\n",
       "\n",
       "**Explanation:**\n",
       "\n",
       "1. The `async` keyword is used to define an async function.\n",
       "2. The `await` keyword is used to pause the execution of the function until it completes.\n",
       "3. The `sleep(2)` function blocks the main thread for 2 seconds.\n",
       "4. After 2 seconds, the `await` keyword resumes the execution of the function.\n",
       "5. The `print(\"Coroutine finished!\")` statement is executed when the coroutine finishes.\n",
       "\n",
       "**Benefits of using coroutines:**\n",
       "\n",
       "* **Non-blocking:** Coroutines do not block the main thread, allowing other tasks to be executed while they are running.\n",
       "* **Efficient:** Coroutines are more efficient than threads, as they do not need to create new threads for each task.\n",
       "* **Easy to use:** Coroutines are easy to use, with the `async` and `await` keywords providing a clear way to define and execute them.\n",
       "\n",
       "**Use cases for coroutines:**\n",
       "\n",
       "* **Long-running tasks:** Coroutines can be used to perform long-running tasks without blocking the main thread.\n",
       "* **UI updates:** Coroutines can be used to perform UI updates without blocking the main thread.\n",
       "* **Asynchronous operations:** Coroutines can be used to perform asynchronous operations, such as network requests.<eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(tokenizer.decode(outputs[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8e2cfbd-4505-48c0-a976-3639c1641733",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>What is coroutine in Python? Explain with an example.\n",
      "\n",
      "Coroutine is a special type of function in Python that allows you to define a function that can be paused and resumed later. This makes it possible to perform long-running tasks without blocking the main thread.\n",
      "\n",
      "Here's an example of how to use coroutines:\n",
      "\n",
      "```python\n",
      "import asyncio\n",
      "\n",
      "async def my_coroutine():\n",
      "    print(\"Starting coroutine...\")\n",
      "    await asyncio.sleep(2)\n",
      "    print(\"Coroutine finished!\")\n",
      "\n",
      "asyncio.run(my_coroutine())\n",
      "```\n",
      "\n",
      "**Output:**\n",
      "\n",
      "```\n",
      "Starting coroutine...\n",
      "Coroutine finished!\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "1. The `async` keyword is used to define an async function.\n",
      "2. The `await` keyword is used to pause the execution of the function until it completes.\n",
      "3. The `sleep(2)` function blocks the main thread for 2 seconds.\n",
      "4. After 2 seconds, the `await` keyword resumes the execution of the function.\n",
      "5. The `print(\"Coroutine finished!\")` statement is executed when the coroutine finishes.\n",
      "\n",
      "**Benefits of using coroutines:**\n",
      "\n",
      "* **Non-blocking:** Coroutines do not block the main thread, allowing other tasks to be executed while they are running.\n",
      "* **Efficient:** Coroutines are more efficient than threads, as they do not need to create new threads for each task.\n",
      "* **Easy to use:** Coroutines are easy to use, with the `async` and `await` keywords providing a clear way to define and execute them.\n",
      "\n",
      "**Use cases for coroutines:**\n",
      "\n",
      "* **Long-running tasks:** Coroutines can be used to perform long-running tasks without blocking the main thread.\n",
      "* **UI updates:** Coroutines can be used to perform UI updates without blocking the main thread.\n",
      "* **Asynchronous operations:** Coroutines can be used to perform asynchronous operations, such as network requests.<eos>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3d304fa-5b81-4f80-95ab-fb1db410609b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference completed and output generated in 0.00023293495178222656s\n"
     ]
    }
   ],
   "source": [
    "# Inferencing using prompt template\n",
    "prompt_inf_start_time = time.time()\n",
    "instruction = \"What is coroutine in Python? Explain with an example.\"\n",
    "prompt = template.format(instruction=instruction)\n",
    "prompt_input_ids = tokenizer(prompt, return_tensors=\"pt\")#.to(\"mps\")\n",
    "out = model.generate(**prompt_input_ids, max_length=1000)\n",
    "print(f\"Inference completed and output generated in {time.time() - prompt_inf_start_time}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "528fed42-77f0-4253-86f4-070955910f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<bos>**<font color='blue'>Instruction:</font>**\n",
       "\n",
       "What is coroutine in Python? Explain with an example.\n",
       "\n",
       "**<font color='green'>Response:</font>**\n",
       "Coroutine is a special type of function in Python that allows you to run multiple functions concurrently without blocking the main thread. This means that you can perform long-running tasks without slowing down the UI or other parts of the application.\n",
       "\n",
       "Here's an example of how to use coroutines:\n",
       "\n",
       "```python\n",
       "import asyncio\n",
       "\n",
       "async def long_running_task():\n",
       "    print(\"Starting long-running task...\")\n",
       "    await asyncio.sleep(2)\n",
       "    print(\"Long-running task finished!\")\n",
       "\n",
       "async def main():\n",
       "    await long_running_task()\n",
       "\n",
       "asyncio.run(main())\n",
       "```\n",
       "\n",
       "**Explanation:**\n",
       "\n",
       "1. The `async` keyword is used to define an async function.\n",
       "2. The `async def` syntax defines a function that returns a coroutine object.\n",
       "3. The `await` keyword is used to pause the execution of the function until it completes.\n",
       "4. The `await long_running_task()` line starts the `long_running_task` function and pauses the execution of `main` until it finishes.\n",
       "5. The `asyncio.run(main())` line starts an asynchronous event loop and runs the `main` function on it.\n",
       "6. The `asyncio.run` function takes an argument `main`, which is the function to run on the event loop.\n",
       "7. The `main` function uses the `async` keyword to define an async function called `long_running_task`.\n",
       "8. The `await` keyword is used to pause the execution of `main` until the `long_running_task` function finishes.\n",
       "9. The `asyncio.run` function starts an asynchronous event loop and runs the `main` function on it.\n",
       "\n",
       "**Output:**\n",
       "\n",
       "When you run the code, you will see the following output:\n",
       "\n",
       "```\n",
       "Starting long-running task...\n",
       "Long-running task finished!\n",
       "```\n",
       "\n",
       "This shows that the `long_running_task` function was executed concurrently with the `main` function, without blocking the main thread.<eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(colorize_text(tokenizer.decode(out[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4ed03d-fcf1-4a05-8d96-e1e3dccf0a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4adcf9dd-d22e-4f6b-a3a1-adf082359412",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a078065d-6977-4d89-b8ee-d18a065d82ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded in 14.285676956176758s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "tokenizer_gpu = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\",\n",
    "                                              device_map=\"mps\",\n",
    "                                              token=os.environ.get(\"hugging_face_token\")\n",
    "                                             )\n",
    "model_gpu = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\",\n",
    "                                                 device_map=\"mps\",\n",
    "                                                 token=os.environ.get(\"hugging_face_token\"),\n",
    "                                                 use_cache=False\n",
    "                                                 # quantization_config=quantization_config\n",
    "                                                )\n",
    "print(f\"Model loaded in {time.time() - start_time}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cf058c9-d628-4e42-859e-317b8ff89ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_gpu = \"What is coroutine in Python? Explain with an executable example.\"\n",
    "input_ids_gpu = tokenizer_gpu(input_text_gpu, return_tensors=\"pt\").to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc459628-f22c-46ad-b094-3b2b3ee2b313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids_gpu['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d45283df-0429-4dd7-a1de-400ac35497d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m inf_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 2\u001b[0m outputs_gpu \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_gpu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_ids_gpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference completed and output generated in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39minf_start_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Programming/llm/venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Programming/llm/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1544\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1527\u001b[0m         input_ids,\n\u001b[1;32m   1528\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1541\u001b[0m     )\n\u001b[1;32m   1542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1543\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/Desktop/Programming/llm/venv/lib/python3.11/site-packages/transformers/generation/utils.py:2467\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2464\u001b[0m         this_peer_finished \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2466\u001b[0m \u001b[38;5;66;03m# stop if we exceed the maximum length\u001b[39;00m\n\u001b[0;32m-> 2467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mstopping_criteria\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   2468\u001b[0m     this_peer_finished \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m this_peer_finished \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m synced_gpus:\n",
      "File \u001b[0;32m~/Desktop/Programming/llm/venv/lib/python3.11/site-packages/transformers/generation/stopping_criteria.py:130\u001b[0m, in \u001b[0;36mStoppingCriteriaList.__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mStoppingCriteriaList\u001b[39;00m(\u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;129m@add_start_docstrings\u001b[39m(STOPPING_CRITERIA_INPUTS_DOCSTRING)\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor, scores: torch\u001b[38;5;241m.\u001b[39mFloatTensor, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28many\u001b[39m(criteria(input_ids, scores, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m criteria \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmax_length\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mint\u001b[39m]:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "inf_start_time = time.time()\n",
    "outputs_gpu = model_gpu.generate(**input_ids_gpu, max_length=1000)\n",
    "print(f\"Inference completed and output generated in {time.time() - inf_start_time}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142d2499-6f46-4ec0-9871-2c94e2fc3eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer_gpu.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d7bbeb-f01b-43d4-aa70-086c950769e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "llm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
