{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c0e3cb4-6b79-4d65-872b-8e0a763f8370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_path = os.path.join(Path.cwd().parent, \".env\")\n",
    "load_dotenv(env_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ce78146-82c6-453f-b8c4-fdf6001f17ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aum/Desktop/Programming/llm/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88ac4857-b465-4f88-9f3f-63d2cf162f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantization_config = BitsAndBytesConfig(load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75d89e6-3983-4081-bace-f6fe1d779a95",
   "metadata": {},
   "source": [
    "### CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3da203be-72e1-41f3-8e1d-f6937dd165f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded in 14.775222063064575s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\",\n",
    "                                          # device_map=\"auto\",\n",
    "                                          token=os.environ.get(\"hugging_face_token\"))\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\",\n",
    "                                             # device_map=\"auto\",\n",
    "                                             token=os.environ.get(\"hugging_face_token\"),\n",
    "                                             use_cache=True\n",
    "                                             # quantization_config=quantization_config\n",
    "                                            )\n",
    "print(f\"Model loaded in {time.time() - start_time}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29f71ba4-a8c9-4bba-a976-ee0a5a3a5e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"What is coroutine in Python? Explain with an example.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")#.to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d71c8c2-0248-470d-bf89-1419f6ef9e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference completed and output generated in 193.66823482513428s\n"
     ]
    }
   ],
   "source": [
    "inf_start_time = time.time()\n",
    "outputs = model.generate(**input_ids, max_length=1000)\n",
    "print(f\"Inference completed and output generated in {time.time() - inf_start_time}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8e2cfbd-4505-48c0-a976-3639c1641733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>What is coroutine in Python? Explain with an example.\n",
      "\n",
      "Coroutine is a special type of function in Python that allows you to define a function that can be paused and resumed later. This makes it possible to perform long-running tasks without blocking the main thread.\n",
      "\n",
      "Here's an example of how to use coroutines:\n",
      "\n",
      "```python\n",
      "import asyncio\n",
      "\n",
      "async def my_coroutine():\n",
      "    print(\"Starting coroutine...\")\n",
      "    await asyncio.sleep(2)\n",
      "    print(\"Coroutine finished!\")\n",
      "\n",
      "asyncio.run(my_coroutine())\n",
      "```\n",
      "\n",
      "**Output:**\n",
      "\n",
      "```\n",
      "Starting coroutine...\n",
      "Coroutine finished!\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "1. The `async` keyword is used to define an async function.\n",
      "2. The `await` keyword is used to pause the execution of the function until it completes.\n",
      "3. The `sleep(2)` function blocks the main thread for 2 seconds.\n",
      "4. After 2 seconds, the `await` keyword resumes the execution of the function.\n",
      "5. The `print(\"Coroutine finished!\")` statement is executed when the coroutine finishes.\n",
      "\n",
      "**Benefits of using coroutines:**\n",
      "\n",
      "* **Non-blocking:** Coroutines do not block the main thread, allowing other tasks to be executed while they are running.\n",
      "* **Efficient:** Coroutines are more efficient than threads, as they do not need to create new threads for each task.\n",
      "* **Easy to use:** Coroutines are easy to use, with the `async` and `await` keywords providing a clear way to define and execute them.\n",
      "\n",
      "**Use cases for coroutines:**\n",
      "\n",
      "* **Long-running tasks:** Coroutines can be used to perform long-running tasks without blocking the main thread.\n",
      "* **UI updates:** Coroutines can be used to perform UI updates without blocking the main thread.\n",
      "* **Asynchronous operations:** Coroutines can be used to perform asynchronous operations, such as network requests.<eos>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4ed03d-fcf1-4a05-8d96-e1e3dccf0a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4adcf9dd-d22e-4f6b-a3a1-adf082359412",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a078065d-6977-4d89-b8ee-d18a065d82ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded in 14.285676956176758s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "tokenizer_gpu = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\",\n",
    "                                              device_map=\"mps\",\n",
    "                                              token=os.environ.get(\"hugging_face_token\")\n",
    "                                             )\n",
    "model_gpu = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\",\n",
    "                                                 device_map=\"mps\",\n",
    "                                                 token=os.environ.get(\"hugging_face_token\"),\n",
    "                                                 use_cache=False\n",
    "                                                 # quantization_config=quantization_config\n",
    "                                                )\n",
    "print(f\"Model loaded in {time.time() - start_time}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cf058c9-d628-4e42-859e-317b8ff89ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_gpu = \"What is coroutine in Python? Explain with an executable example.\"\n",
    "input_ids_gpu = tokenizer_gpu(input_text_gpu, return_tensors=\"pt\").to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc459628-f22c-46ad-b094-3b2b3ee2b313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids_gpu['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d45283df-0429-4dd7-a1de-400ac35497d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m inf_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 2\u001b[0m outputs_gpu \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_gpu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_ids_gpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference completed and output generated in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39minf_start_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Programming/llm/venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Programming/llm/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1544\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1527\u001b[0m         input_ids,\n\u001b[1;32m   1528\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1541\u001b[0m     )\n\u001b[1;32m   1542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1543\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/Desktop/Programming/llm/venv/lib/python3.11/site-packages/transformers/generation/utils.py:2467\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2464\u001b[0m         this_peer_finished \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2466\u001b[0m \u001b[38;5;66;03m# stop if we exceed the maximum length\u001b[39;00m\n\u001b[0;32m-> 2467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mstopping_criteria\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   2468\u001b[0m     this_peer_finished \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m this_peer_finished \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m synced_gpus:\n",
      "File \u001b[0;32m~/Desktop/Programming/llm/venv/lib/python3.11/site-packages/transformers/generation/stopping_criteria.py:130\u001b[0m, in \u001b[0;36mStoppingCriteriaList.__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mStoppingCriteriaList\u001b[39;00m(\u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;129m@add_start_docstrings\u001b[39m(STOPPING_CRITERIA_INPUTS_DOCSTRING)\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor, scores: torch\u001b[38;5;241m.\u001b[39mFloatTensor, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28many\u001b[39m(criteria(input_ids, scores, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m criteria \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmax_length\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mint\u001b[39m]:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "inf_start_time = time.time()\n",
    "outputs_gpu = model_gpu.generate(**input_ids_gpu, max_length=1000)\n",
    "print(f\"Inference completed and output generated in {time.time() - inf_start_time}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142d2499-6f46-4ec0-9871-2c94e2fc3eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer_gpu.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d7bbeb-f01b-43d4-aa70-086c950769e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "llm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
