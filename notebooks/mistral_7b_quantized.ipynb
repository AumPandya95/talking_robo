{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42b76118-5cc7-4aa7-85f4-482822e67b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama, LlamaTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model = \"../model/mistral-7b-instruct-v0.1.Q6_K.gguf\"  # instruction model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9ec3e20-f15f-429a-b9f7-31fc3880c328",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../model/mistral-7b-instruct-v0.1.Q6_K.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q6_K:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 5.53 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =   341.33 MiB, (  341.39 / 10922.67)\n",
      "llm_load_tensors: offloading 2 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 2/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  5666.09 MiB\n",
      "llm_load_tensors:      Metal buffer size =   341.32 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1 Pro\n",
      "ggml_metal_init: picking default device: Apple M1 Pro\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/aum/Desktop/Programming/llm/venv/lib/python3.11/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M1 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =   960.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =    64.00 MiB, (  406.95 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    24.04 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     0.02 MiB, (  406.97 / 10922.67)\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   607.20 MiB, ( 1014.16 / 10922.67)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   607.20 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   589.60 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 5\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '18', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.1'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"insensitive string 'Mistral'. Add docstring as well.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = Llama(\n",
    "    model_path=model, \n",
    "    n_ctx=8192, \n",
    "    n_batch=512, \n",
    "    n_threads=7, \n",
    "    n_gpu_layers=2, \n",
    "    verbose=True, \n",
    "    seed=42\n",
    ")\n",
    "system = \"\"\"\n",
    "Follow the instructions below to complete the task.\n",
    "\"\"\"\n",
    "\n",
    "user = f\"Create a python script to scan a directory and return the files of the directory that match a particular case \"\n",
    "f\"insensitive string 'Mistral'. Add docstring as well.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e58bbfa-bb37-45bf-b394-60db4ec6786b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 28705,\n",
       " 13,\n",
       " 28093,\n",
       " 272,\n",
       " 11382,\n",
       " 3624,\n",
       " 298,\n",
       " 4160,\n",
       " 272,\n",
       " 3638,\n",
       " 28723,\n",
       " 13,\n",
       " 3998,\n",
       " 264,\n",
       " 21966,\n",
       " 6767,\n",
       " 298,\n",
       " 12794,\n",
       " 264,\n",
       " 9118,\n",
       " 304,\n",
       " 604,\n",
       " 272,\n",
       " 5373,\n",
       " 302,\n",
       " 272,\n",
       " 9118,\n",
       " 369,\n",
       " 2918,\n",
       " 264,\n",
       " 2830,\n",
       " 1222,\n",
       " 28705]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.tokenize((system+user).encode(\"utf-8\", errors=\"ignore\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09473b0f-71e2-4da9-a949-b8a7fb7742b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 28705,\n",
       " 13,\n",
       " 28093,\n",
       " 272,\n",
       " 11382,\n",
       " 3624,\n",
       " 298,\n",
       " 4160,\n",
       " 272,\n",
       " 3638,\n",
       " 28723,\n",
       " 13,\n",
       " 3998,\n",
       " 264,\n",
       " 21966,\n",
       " 6767,\n",
       " 298,\n",
       " 12794,\n",
       " 264,\n",
       " 9118,\n",
       " 304,\n",
       " 604,\n",
       " 272,\n",
       " 5373,\n",
       " 302,\n",
       " 272,\n",
       " 9118,\n",
       " 369,\n",
       " 2918,\n",
       " 264,\n",
       " 2830,\n",
       " 1222,\n",
       " 28705]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LlamaTokenizer(llm).encode(system + user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "959131e3-91f0-4b0e-aa36-96a601fcf935",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0058cadf-9ab2-4f7b-889a-ccccb3c3e611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'What is your favourite condiment?'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
       " {'role': 'user', 'content': 'Do you have mayonnaise recipes?'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.create_chat_completion("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93184374-d060-403f-afca-31e8d17da717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12824.35 ms\n",
      "llama_print_timings:      sample time =      17.72 ms /   210 runs   (    0.08 ms per token, 11853.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   17551.17 ms /   210 runs   (   83.58 ms per token,    11.97 tokens per second)\n",
      "llama_print_timings:       total time =   17906.92 ms /   211 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-33d50ad9-6a6b-4c22-9e8f-9555e61da741',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1708546358,\n",
       " 'model': '../model/mistral-7b-instruct-v0.1.Q6_K.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': \"Of course! Here's a simple recipe for homemade mayonnaise that you can try:\\n\\nIngredients:\\n\\n* 2 large egg yolks\\n* 1 tablespoon Dijon mustard\\n* 1 tablespoon white wine vinegar\\n* 1 tablespoon honey\\n* 1 cup vegetable oil\\n* Salt and pepper to taste\\n\\nInstructions:\\n\\n1. In a medium-sized bowl, whisk together the egg yolks, Dijon mustard, white wine vinegar, and honey until well combined.\\n2. Slowly drizzle in the vegetable oil while whisking constantly until the mixture thickens and emulsifies.\\n3. Season with salt and pepper to taste.\\n4. Serve immediately or transfer to an airtight container and refrigerate until ready to use.\\n\\nThat's it! Enjoy your homemade mayonnaise on your favorite sandwich or as a dip for your veggies.\"},\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 76, 'completion_tokens': 209, 'total_tokens': 285}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.create_chat_completion(messages, max_tokens=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "181658be-0c5f-46a0-8f1a-2f8e324da87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12824.35 ms\n",
      "llama_print_timings:      sample time =      14.32 ms /   144 runs   (    0.10 ms per token, 10055.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12823.71 ms /    43 tokens (  298.23 ms per token,     3.35 tokens per second)\n",
      "llama_print_timings:        eval time =   12117.31 ms /   143 runs   (   84.74 ms per token,    11.80 tokens per second)\n",
      "llama_print_timings:       total time =   25163.57 ms /   186 tokens\n"
     ]
    }
   ],
   "source": [
    "a = llm.create_completion(f\"<s>[INST] {system} [/INST]</s>{user}\", echo=True, stream=False, max_tokens=4096)['choices'][0]['text'] #.replace(message, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62d3178a-59d4-46d0-a4f4-3f2260964b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] \n",
      "Follow the instructions below to complete the task.\n",
      " [/INST]</s>Create a python script to scan a directory and return the files of the directory that match a particular case \n",
      "insensitive pattern.\n",
      "\n",
      "```python\n",
      "import os\n",
      "def find_files(directory, pattern):\n",
      "    matching_files = []\n",
      "    for file in os.listdir(directory):\n",
      "        if file.lower().endswith(pattern.lower()):\n",
      "            matching_files.append(file)\n",
      "    return matching_files\n",
      "```\n",
      "Usage:\n",
      "```python\n",
      "files = find_files('directory', 'extension')\n",
      "print(files)\n",
      "```\n",
      "Note: This will only work for directories with a single level directory structure. If the directory contains nested directories, you will need to modify the code to recursively search for files.\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dce5814-fda1-4e16-a647-95451967964d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12824.35 ms\n",
      "llama_print_timings:      sample time =      58.06 ms /   574 runs   (    0.10 ms per token,  9886.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2665.45 ms /    33 tokens (   80.77 ms per token,    12.38 tokens per second)\n",
      "llama_print_timings:        eval time =   41401.50 ms /   573 runs   (   72.25 ms per token,    13.84 tokens per second)\n",
      "llama_print_timings:       total time =   45065.32 ms /   606 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Follow the instructions below to complete the task.\n",
      "Create a python script to scan a directory and return the files of the directory that match a particular case  sensitive pattern and have not been modified in the last 7 days.\n",
      "1. First, we will import the os module which provides access to the operating system directory structure and the time module which provides time manipulation functions. We also import the shutil module for moving files from one place to another.\n",
      "```python\n",
      "import os\n",
      "import time\n",
      "import shutil\n",
      "```\n",
      "2. Next we define the function `scan_directory` which takes two arguments- `directory_path` and `file_pattern`. `directory_path` is the path of the directory whose files we want to scan and `file_pattern` is the pattern we want to search for in the file names. \n",
      "```python\n",
      "def scan_directory(directory_path, file_pattern):\n",
      "```\n",
      "3. Inside the function `scan_directory`, we use a `for` loop to iterate over all the files in the specified directory using `os.listdir()` method. We check if each file matches the pattern using `file_pattern` regular expression using the `re` module. If it matches, we check if it has been modified within the last 7 days using `os.path.getmtime()` method. If it has not been modified within the last 7 days, we append it to a list called `unmodified_files`.\n",
      "```python\n",
      "for file in os.listdir(directory_path):\n",
      "    if re.search(file_pattern, file):\n",
      "        if os.path.getmtime(os.path.join(directory_path, file)) < time.time() - 7*24*60*60: # check if file has been modified within last 7 days\n",
      "            unmodified_files.append(file)\n",
      "```\n",
      "4. After the loop, we print the list of unmodified files using `print()` function.\n",
      "```python\n",
      "print(unmodified_files)\n",
      "```\n",
      "5. Finally, we define a function `main()` which calls the `scan_directory()` function with the specified arguments and prints the list of unmodified files.\n",
      "```python\n",
      "def main():\n",
      "    directory_path = \"/path/to/directory\"\n",
      "    file_pattern = r\"\\bexample\\b\" # pattern to search for in file names\n",
      "    scan_directory(directory_path, file_pattern)\n",
      "```\n",
      "Replace `/path/to/directory` with the path of the directory whose files you want to scan and `r\"\\bexample\\b\"` with the regular expression pattern you want to search for in file names. Run the `main()` function to get the list of unmodified files.\n"
     ]
    }
   ],
   "source": [
    "b = llm.create_completion(system+user, echo=True, stream=False, max_tokens=4096)['choices'][0]['text'] #.replace(message, '')\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b099d4fc-b2fc-4b5a-be8f-27959a9c00c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    9920.47 ms\n",
      "llama_print_timings:      sample time =      23.09 ms /   224 runs   (    0.10 ms per token,  9699.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3743.07 ms /    40 tokens (   93.58 ms per token,    10.69 tokens per second)\n",
      "llama_print_timings:        eval time =   15830.62 ms /   223 runs   (   70.99 ms per token,    14.09 tokens per second)\n",
      "llama_print_timings:       total time =   19946.21 ms /   263 tokens\n"
     ]
    }
   ],
   "source": [
    "message = f\"<s>[INST] {system} [/INST]</s>{user}\"\n",
    "output = llm.create_completion(message, echo=True, stream=False, max_tokens=4096)\n",
    "output_cleaned = output['choices'][0]['text'].replace(message, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b50d45bd-7268-499e-8950-2b8dacd9dfa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4442618000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4905676b-3c3a-4787-9d26-9bca37e5c004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_cpp._internals._LlamaModel at 0x282cef350>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm._model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6568c7aa-10dd-45bd-b1b0-a92254f3f070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_cpp._internals._LlamaModel at 0x282cef350>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm._model.tokenize(system+user, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "96be89e5-3977-4939-b76e-1257e2a23d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt_tokens': 40, 'completion_tokens': 223, 'total_tokens': 263}\n",
      "\n",
      "\n",
      "1. Import os module for operating system functionality.\n",
      "2. Define the function scan_directory with two parameters: directory_path (string) and pattern (string).\n",
      "3. Use the os.listdir() method to get a list of all files in the directory.\n",
      "4. Use the filter() method to filter out the files that don't match the pattern.\n",
      "5. Use the map() method to convert the file names into absolute paths using the join() method of the os module.\n",
      "6. Return the list of absolute paths of the matching files.\n",
      "\n",
      "Example:\n",
      "\n",
      "```python\n",
      "import os\n",
      "\n",
      "def scan_directory(directory_path, pattern):\n",
      "    matching_files = []\n",
      "    for file in os.listdir(directory_path):\n",
      "        if file.lower() == pattern.lower():\n",
      "            matching_files.append(os.path.join(directory_path, file))\n",
      "    return matching_files\n",
      "\n",
      "print(scan_directory('path/to/directory', 'example'))\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(output['usage'])\n",
    "print(output_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85b0c210-8306-4a1b-8e42-9b9a253ac61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'object', 'created', 'model', 'choices', 'usage'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddd4ccd1-5fc3-48e5-acf2-b170a9d7b55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt_tokens': 60, 'completion_tokens': 4096, 'total_tokens': 4156}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['usage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971ed617-3a41-4234-85cd-b1bf65a80c83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ee1616b9-6848-4aa9-a491-650057e08ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bdf6f5c3-63c2-45c4-8838-e66fc99767b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(directory: str, file_search_term: Optional[str] = None) -> List[str]:\n",
    "    \"\"\"\n",
    "    Scans a given directory and returns a list of files that match a given search term.\n",
    "    :param directory: str - The path of the directory to be scanned\n",
    "    :param file_search_term: Optional[str] - The search term to be matched in file names. If None, all files will be returned\n",
    "    :return: List[str] - A list of file names matching the search term or all files in the directory if file_search_term is None\n",
    "    \"\"\"\n",
    "    file_list = []\n",
    "    if file_search_term is None:\n",
    "        for file in os.listdir(directory):\n",
    "            file_list.append(file)\n",
    "    else:\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if file_search_term in file:\n",
    "                    file_list.append(os.path.join(root, file))\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d1b8facf-f56e-4fbf-851c-65a925ebaa37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_files(\"model/\", \"Mistral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "86c92348-79ca-4ea3-aae7-562873eec704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "def find_files_with_string(directory, pattern):\n",
    "    \"\"\"\n",
    "    This function takes in two arguments, a directory and a pattern, and returns a list of all\n",
    "    the files in that directory which contain the given pattern in their filename or content.\n",
    "    \"\"\"\n",
    "    file_list = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            filepath = os.path.join(root, filename)\n",
    "            if re.search(pattern, filename, re.IGNORECASE):\n",
    "                file_list.append(filepath)\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5f79574a-d1b1-4b2d-b075-a7f86775ef18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model/mistral-7b-instruct-v0.1.Q6_K.gguf']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_files_with_string(\"model/\", \"Mistral\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "llm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
