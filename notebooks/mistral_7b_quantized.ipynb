{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42b76118-5cc7-4aa7-85f4-482822e67b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aum/Desktop/Programming/llm/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama, LlamaTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model = \"../model/mistral-7b-instruct-v0.1.Q6_K.gguf\"  # instruction model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9ec3e20-f15f-429a-b9f7-31fc3880c328",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../model/mistral-7b-instruct-v0.1.Q6_K.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q6_K:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 5.53 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =   341.33 MiB, ( 1355.48 / 10922.67)\n",
      "llm_load_tensors: offloading 2 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 2/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  5666.09 MiB\n",
      "llm_load_tensors:      Metal buffer size =   341.32 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1 Pro\n",
      "ggml_metal_init: picking default device: Apple M1 Pro\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/aum/Desktop/Programming/llm/venv/lib/python3.11/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M1 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =   960.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =    64.00 MiB, ( 1419.73 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    24.04 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     0.02 MiB, ( 1419.75 / 10922.67)\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   607.20 MiB, ( 2026.94 / 10922.67)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   607.20 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   589.60 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 5\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '18', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.1'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"insensitive string 'Mistral'. Add docstring as well.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = Llama(\n",
    "    model_path=model, \n",
    "    n_ctx=8192, \n",
    "    n_batch=512, \n",
    "    n_threads=7, \n",
    "    n_gpu_layers=2, \n",
    "    verbose=True, \n",
    "    seed=42\n",
    ")\n",
    "system = \"Follow the instructions below to complete the task.\"\n",
    "\n",
    "user = f\"Create a python script to scan a directory and return the files of the directory that match a particular case \"\n",
    "f\"insensitive string 'Mistral'. Add docstring as well.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e58bbfa-bb37-45bf-b394-60db4ec6786b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 28705,\n",
       " 13,\n",
       " 28093,\n",
       " 272,\n",
       " 11382,\n",
       " 3624,\n",
       " 298,\n",
       " 4160,\n",
       " 272,\n",
       " 3638,\n",
       " 28723,\n",
       " 13,\n",
       " 3998,\n",
       " 264,\n",
       " 21966,\n",
       " 6767,\n",
       " 298,\n",
       " 12794,\n",
       " 264,\n",
       " 9118,\n",
       " 304,\n",
       " 604,\n",
       " 272,\n",
       " 5373,\n",
       " 302,\n",
       " 272,\n",
       " 9118,\n",
       " 369,\n",
       " 2918,\n",
       " 264,\n",
       " 2830,\n",
       " 1222,\n",
       " 28705]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.tokenize((system+user).encode(\"utf-8\", errors=\"ignore\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09473b0f-71e2-4da9-a949-b8a7fb7742b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 28705,\n",
       " 13,\n",
       " 28093,\n",
       " 272,\n",
       " 11382,\n",
       " 3624,\n",
       " 298,\n",
       " 4160,\n",
       " 272,\n",
       " 3638,\n",
       " 28723,\n",
       " 13,\n",
       " 3998,\n",
       " 264,\n",
       " 21966,\n",
       " 6767,\n",
       " 298,\n",
       " 12794,\n",
       " 264,\n",
       " 9118,\n",
       " 304,\n",
       " 604,\n",
       " 272,\n",
       " 5373,\n",
       " 302,\n",
       " 272,\n",
       " 9118,\n",
       " 369,\n",
       " 2918,\n",
       " 264,\n",
       " 2830,\n",
       " 1222,\n",
       " 28705]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LlamaTokenizer(llm).encode(system + user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "959131e3-91f0-4b0e-aa36-96a601fcf935",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93184374-d060-403f-afca-31e8d17da717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13503.27 ms\n",
      "llama_print_timings:      sample time =      20.64 ms /   225 runs   (    0.09 ms per token, 10900.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =   13502.87 ms /    76 tokens (  177.67 ms per token,     5.63 tokens per second)\n",
      "llama_print_timings:        eval time =  355062.95 ms /   224 runs   ( 1585.10 ms per token,     0.63 tokens per second)\n",
      "llama_print_timings:       total time =  369047.35 ms /   300 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-70a5d3f9-d581-4e6a-8706-c14d2370ea7a',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1711735424,\n",
       " 'model': '../model/mistral-7b-instruct-v0.1.Q6_K.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': \"Of course! Here's a simple recipe for homemade mayonnaise that you can try:\\n\\nIngredients:\\n\\n* 2 large egg yolks\\n* 1 tablespoon Dijon mustard\\n* 1 tablespoon white wine vinegar\\n* 1 teaspoon salt\\n* 1/4 teaspoon black pepper\\n* 1 cup vegetable oil\\n\\nInstructions:\\n\\n1. In a medium-sized bowl, whisk together the egg yolks, Dijon mustard, white wine vinegar, salt, and black pepper until well combined.\\n2. Slowly drizzle in the vegetable oil while whisking constantly until the mixture thickens and emulsifies.\\n3. Taste and adjust seasoning as needed.\\n4. Cover the bowl with plastic wrap and refrigerate for at least an hour before serving.\\n\\nThat's it! You can use this homemade mayonnaise as a base for many classic dishes like salads, sandwiches, and pasta dishes. Enjoy!\"},\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 76, 'completion_tokens': 224, 'total_tokens': 300}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.create_chat_completion(messages, max_tokens=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "181658be-0c5f-46a0-8f1a-2f8e324da87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13503.27 ms\n",
      "llama_print_timings:      sample time =      36.51 ms /   368 runs   (    0.10 ms per token, 10079.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2418.88 ms /    42 tokens (   57.59 ms per token,    17.36 tokens per second)\n",
      "llama_print_timings:        eval time =   24925.67 ms /   367 runs   (   67.92 ms per token,    14.72 tokens per second)\n",
      "llama_print_timings:       total time =   27972.07 ms /   409 tokens\n"
     ]
    }
   ],
   "source": [
    "a = llm.create_completion(f\"<s>[INST] {system} [/INST]</s>{user}\", echo=True, stream=False, max_tokens=4096)['choices'][0]['text'] #.replace(message, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62d3178a-59d4-46d0-a4f4-3f2260964b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] \n",
      "Follow the instructions below to complete the task.\n",
      " [/INST]</s>Create a python script to scan a directory and return the files of the directory that match a particular case \n",
      "in pattern\n",
      "\n",
      "```python\n",
      "import os\n",
      "from pathlib import Path\n",
      "\n",
      "def find_files(directory, pattern):\n",
      "    \"\"\"\n",
      "    This function takes in a directory and pattern as inputs,\n",
      "    it then scans the directory and returns the file names that match the pattern\n",
      "    \"\"\"\n",
      "    matching_files = []\n",
      "    for item in os.listdir(directory):\n",
      "        file_path = os.path.join(directory, item)\n",
      "        if item.isfile() and item.lower() in pattern.lower():\n",
      "            matching_files.append(file_path)\n",
      "    return matching_files\n",
      "\n",
      "# example usage\n",
      "directory = \"/path/to/directory\"\n",
      "pattern = \"example\"\n",
      "\n",
      "files = find_files(directory, pattern)\n",
      "print(files)\n",
      "```\n",
      "This script uses the `os` module for interacting with the file system, and the `pathlib` module for working with paths and directories in Python 3. It defines a function `find_files(directory, pattern)` that takes in two arguments: `directory`, which is the directory to be scanned, and `pattern`, which is a string that represents the pattern to search for in the file names. The function uses a `for` loop to iterate over all the items in the directory and checks if the item is a file and if its `lower()` value is in the `lower()` value of `pattern`. If a file matches the pattern, it is added to the `matching_files` list, which is returned at the end. Finally, the example usage demonstrates how to use the function by providing a directory path and a pattern, and then printing out the matching file paths.\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dce5814-fda1-4e16-a647-95451967964d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13503.27 ms\n",
      "llama_print_timings:      sample time =      41.68 ms /   401 runs   (    0.10 ms per token,  9621.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2304.26 ms /    33 tokens (   69.83 ms per token,    14.32 tokens per second)\n",
      "llama_print_timings:        eval time =   27794.41 ms /   400 runs   (   69.49 ms per token,    14.39 tokens per second)\n",
      "llama_print_timings:       total time =   30801.61 ms /   433 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Follow the instructions below to complete the task.\n",
      "Create a python script to scan a directory and return the files of the directory that match a particular case \n",
      "pattern. Here is an example:\n",
      "```python\n",
      "import os\n",
      "\n",
      "def find_files(directory, pattern):\n",
      "    matching_files = []\n",
      "    for root, _, files in os.walk(directory):\n",
      "        for file in files:\n",
      "            if file.lower().endswith(\".txt\"):\n",
      "                if file.lower() == pattern.lower():\n",
      "                    matching_files.append(os.path.join(root, file))\n",
      "    return matching_files\n",
      "\n",
      "# Example usage\n",
      "print(find_files(\"/path/to/directory\", \"example.txt\"))\n",
      "```\n",
      "The `find_files` function takes in two arguments - `directory`, which is the path to the directory to be scanned, and `pattern`, which is the name of the file to be searched for in the directory. The function returns a list of all the files in the directory that have the same name as `pattern`.\n",
      "The function uses the `os.walk()` function to traverse the directory tree recursively, starting from `directory`. For each subdirectory, it iterates over the list of files in the subdirectory using `files`. For each file in the subdirectory, it checks if the file has a `.txt` extension using `file.lower().endswith(\".txt\")`, and if its name is equal to `pattern` using `file.lower() == pattern.lower()`. If both conditions are true, the file path is added to the `matching_files` list using `os.path.join()`. Finally, the function returns the `matching_files` list.\n",
      "The example usage at the end of the code calls the `find_files()` function with `directory` set to `\"/path/to/directory\"` and `pattern` set to `\"example.txt\"`. It prints the resulting list of file paths.\n"
     ]
    }
   ],
   "source": [
    "b = llm.create_completion(system+user, echo=True, stream=False, max_tokens=4096)['choices'][0]['text'] #.replace(message, '')\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b099d4fc-b2fc-4b5a-be8f-27959a9c00c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13503.27 ms\n",
      "llama_print_timings:      sample time =      12.81 ms /   122 runs   (    0.10 ms per token,  9526.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2313.63 ms /    42 tokens (   55.09 ms per token,    18.15 tokens per second)\n",
      "llama_print_timings:        eval time =    8100.84 ms /   121 runs   (   66.95 ms per token,    14.94 tokens per second)\n",
      "llama_print_timings:       total time =   10611.20 ms /   163 tokens\n"
     ]
    }
   ],
   "source": [
    "message = f\"<s>[INST] {system} [/INST]</s>{user}\"\n",
    "output = llm.create_completion(message, echo=True, stream=False, max_tokens=4096)\n",
    "output_cleaned = output['choices'][0]['text'].replace(message, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b50d45bd-7268-499e-8950-2b8dacd9dfa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6049099584"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4905676b-3c3a-4787-9d26-9bca37e5c004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_cpp._internals._LlamaModel at 0x154480290>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm._model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04c69eb9-d3c5-4c01-b9ed-343812588fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = system+user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6568c7aa-10dd-45bd-b1b0-a92254f3f070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 9822,\n",
       " 272,\n",
       " 11382,\n",
       " 3624,\n",
       " 298,\n",
       " 4160,\n",
       " 272,\n",
       " 3638,\n",
       " 28723,\n",
       " 3998,\n",
       " 264,\n",
       " 21966,\n",
       " 6767,\n",
       " 298,\n",
       " 12794,\n",
       " 264,\n",
       " 9118,\n",
       " 304,\n",
       " 604,\n",
       " 272,\n",
       " 5373,\n",
       " 302,\n",
       " 272,\n",
       " 9118,\n",
       " 369,\n",
       " 2918,\n",
       " 264,\n",
       " 2830,\n",
       " 1222,\n",
       " 28705]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm._model.tokenize(prompt.encode(\"utf-8\", errors=\"ignore\"), add_bos=True, special=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96be89e5-3977-4939-b76e-1257e2a23d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt_tokens': 43, 'completion_tokens': 121, 'total_tokens': 164}\n",
      "\n",
      "sensitivity pattern.\n",
      "```python\n",
      "import os\n",
      "def find_files(directory, pattern):\n",
      "    \"\"\"\n",
      "    This function scans a directory and returns the files that match the given pattern.\n",
      "    \"\"\"\n",
      "    for root, dirs, files in os.walk(directory):\n",
      "        for file in files:\n",
      "            if file.lower() == pattern.lower():\n",
      "                yield os.path.join(root, file)\n",
      "\n",
      "# Example usage:\n",
      "\n",
      "find_files('/path/to/directory', '*.txt')\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(output['usage'])\n",
    "print(output_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85b0c210-8306-4a1b-8e42-9b9a253ac61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'object', 'created', 'model', 'choices', 'usage'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ddd4ccd1-5fc3-48e5-acf2-b170a9d7b55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt_tokens': 43, 'completion_tokens': 121, 'total_tokens': 164}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['usage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971ed617-3a41-4234-85cd-b1bf65a80c83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee1616b9-6848-4aa9-a491-650057e08ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bdf6f5c3-63c2-45c4-8838-e66fc99767b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(directory: str, file_search_term: Optional[str] = None) -> List[str]:\n",
    "    \"\"\"\n",
    "    Scans a given directory and returns a list of files that match a given search term.\n",
    "    :param directory: str - The path of the directory to be scanned\n",
    "    :param file_search_term: Optional[str] - The search term to be matched in file names. If None, all files will be returned\n",
    "    :return: List[str] - A list of file names matching the search term or all files in the directory if file_search_term is None\n",
    "    \"\"\"\n",
    "    file_list = []\n",
    "    if file_search_term is None:\n",
    "        for file in os.listdir(directory):\n",
    "            file_list.append(file)\n",
    "    else:\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if file_search_term in file:\n",
    "                    file_list.append(os.path.join(root, file))\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d1b8facf-f56e-4fbf-851c-65a925ebaa37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_files(\"../model/\", \"Mistral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "86c92348-79ca-4ea3-aae7-562873eec704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "def find_files_with_string(directory, pattern):\n",
    "    \"\"\"\n",
    "    This function takes in two arguments, a directory and a pattern, and returns a list of all\n",
    "    the files in that directory which contain the given pattern in their filename or content.\n",
    "    \"\"\"\n",
    "    file_list = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            filepath = os.path.join(root, filename)\n",
    "            if re.search(pattern, filename, re.IGNORECASE):\n",
    "                file_list.append(filepath)\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5f79574a-d1b1-4b2d-b075-a7f86775ef18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../model/mistral-7b-instruct-v0.1.Q6_K.gguf']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_files_with_string(\"../model/\", \"Mistral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdb445c-b0fd-419a-a49a-54d3fdd65699",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "llm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
